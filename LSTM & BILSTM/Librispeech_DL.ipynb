{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Extraction des Features\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JDcF5TFRYRsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_lstm_features(file_path,\n",
        "                          target_sr=16000,\n",
        "                          max_duration=4.0,\n",
        "                          n_mfcc=13,\n",
        "                          hop_length=160,\n",
        "                          n_fft=1024,\n",
        "                          trim_silence=True,\n",
        "                          top_db=25):\n",
        "    \"\"\"\n",
        "    Convertit un fichier audio en caractéristiques MFCC pour un modèle LSTM.\n",
        "    Retourne un tableau de forme (time_steps, n_features) pour une entrée LSTM.\n",
        "\n",
        "    Args:\n",
        "        file_path: Chemin vers le fichier audio\n",
        "        target_sr: Taux d'échantillonnage cible\n",
        "        max_duration: Durée maximale en secondes\n",
        "        n_mfcc: Nombre de coefficients MFCC\n",
        "        hop_length: Nombre d'échantillons entre les trames successives\n",
        "        n_fft: Taille de la fenêtre FFT\n",
        "        trim_silence: Si True, supprime les silences\n",
        "        top_db: Seuil pour la suppression du silence\n",
        "\n",
        "    Returns:\n",
        "        features: Tableau numpy de forme (time_steps, n_mfcc)\n",
        "    \"\"\"\n",
        "\n",
        "    # Chargement et prétraitement audio\n",
        "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
        "    audio = librosa.util.normalize(audio)\n",
        "\n",
        "    if trim_silence:\n",
        "        audio = librosa.effects.trim(audio, top_db=top_db)[0]\n",
        "\n",
        "    # Découpage/Padding pour longueur fixe\n",
        "    target_length = int(max_duration * target_sr)\n",
        "    if len(audio) > target_length:\n",
        "        audio = audio[:target_length]\n",
        "    else:\n",
        "        padding = target_length - len(audio)\n",
        "        audio = np.pad(audio, (0, padding), mode='constant')\n",
        "\n",
        "    # Extraction MFCC\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr,\n",
        "                               n_mfcc=n_mfcc,\n",
        "                               hop_length=hop_length,\n",
        "                               n_fft=n_fft,\n",
        "                               fmin=50,\n",
        "                               fmax=8000)\n",
        "\n",
        "    # Normalisation par trame (le long des features)\n",
        "    mfcc = (mfcc - np.mean(mfcc, axis=0)) / (np.std(mfcc, axis=0) + 1e-8)\n",
        "\n",
        "    # Transposition pour avoir la forme (time_steps, n_features)\n",
        "    mfcc = mfcc.T  # Shape: (time_steps, n_mfcc)\n",
        "\n",
        "    return mfcc"
      ],
      "metadata": {
        "id": "xyfzIQioYKvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def process_files(df):\n",
        "    results = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        try:\n",
        "            features = audio_to_lstm_features(row['path']  , trim_silence=True)\n",
        "            results.append({\n",
        "                'path': row['path'],\n",
        "                'features': features\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur {row['path']}: {str(e)}\")\n",
        "    return results\n",
        "\n",
        "# Exécution\n",
        "features_data = process_files(df)"
      ],
      "metadata": {
        "id": "RHsrfBkbYPmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Séparation des données"
      ],
      "metadata": {
        "id": "qND_jqc4YfE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([x['features'] for x in features_data])  # Features audio\n",
        "y = df['speaker_id'].values"
      ],
      "metadata": {
        "id": "QPmBctMQZkvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model.keras',\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "VIFpMTcyZlbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels correctement\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)  # Utilisation correcte de LabelEncoder\n",
        "\n",
        "# Séparation des données en ensembles d'entraînement, validation et test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Affichage des dimensions des ensembles\n",
        "print(\"Training Data Shape:\", X_train.shape)\n",
        "print(\"Validation Data Shape:\", X_val.shape)\n",
        "print(\"Test Data Shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "4kq_7wLyZp3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lstm : une seule couche\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n11C1F4fX4n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U57gaU90W6Fc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model_lstm = Sequential([\n",
        "    LSTM(128, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),  # input shape = (T, 27)\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(y_categorical.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "RbbutvYqXscP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history4 = model_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=300,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "j-k92UoTXma8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "test_loss4, test_accuracy4 = model_lstm.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy4 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "jqq1q6IhXoHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4IJqgsviX3uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Accuracy Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd plot\n",
        "plt.plot(history4.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history4.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curve')"
      ],
      "metadata": {
        "id": "FcLgtH3FXqXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st plot\n",
        "plt.plot(history4.history['loss'], label='Training Loss')\n",
        "plt.plot(history4.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curve')"
      ],
      "metadata": {
        "id": "Pz3pHy8gXvVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lstm : 2 couches"
      ],
      "metadata": {
        "id": "ScL-HMeZZ4gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "model_lstm2_4 = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    BatchNormalization(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(y_categorical.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm2_4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "lUUwl-ZqZ7Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2_4 = model_lstm2_4.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "EbxUoN31bON8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "test_loss4, test_accuracy4 = model_lstm2_4.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy4 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "tw3CDcGqbXLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Accuracy Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd plot\n",
        "plt.plot(history2_4.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history2_4.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curve')"
      ],
      "metadata": {
        "id": "Yb4ix2i1bY9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st plot\n",
        "plt.plot(history2_4.history['loss'], label='Training Loss')\n",
        "plt.plot(history2_4.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curve')"
      ],
      "metadata": {
        "id": "Fjp6PnPabfSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_lstm = model_lstm2_4.predict(X_test)\n",
        "y_pred_classes_lstm = np.argmax(y_pred_lstm , axis=1)\n",
        "y_true_lstm = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Rapport de classification\n",
        "target_names = [f\"Classe {i}\" for i in range(251)]\n",
        "print(classification_report(y_true_lstm, y_pred_classes_lstm , target_names=target_names))\n",
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_true_lstm , y_pred_classes_lstm , average='macro')"
      ],
      "metadata": {
        "id": "Bt1fxSE0bgEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bi-Lstm"
      ],
      "metadata": {
        "id": "fu4_o9zEcKav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Bidirectional(\n",
        "        LSTM(128, return_sequences=True),   input_shape=(401, 13)  ),\n",
        "\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Bidirectional(LSTM(64)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(251, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "bkAChOvwcMc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelbi3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "J3tmgNBxcdYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelbi3.summary()"
      ],
      "metadata": {
        "id": "Fbh4NGoQchKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historybic3 = modelbi3.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "U5HjRsMHcjaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "test_lossbi3, test_accuracybi3 = modelbi3.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracybi3 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "3gGh89bUcj-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot Loss Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st plot\n",
        "plt.plot(historybic3.history['loss'], label='Training Loss')\n",
        "plt.plot(historybic3.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curve')"
      ],
      "metadata": {
        "id": "dbcuki7hcnBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd plot\n",
        "plt.plot(historybic3.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(historybic3.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curve')"
      ],
      "metadata": {
        "id": "1CioYshucq0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_true, y_pred_classes, average='macro')"
      ],
      "metadata": {
        "id": "bpkqiinwcs7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "y_pred = modelbi3.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Rapport de classification\n",
        "target_names = [f\"Classe {i}\" for i in range(251)]\n",
        "print(classification_report(y_true, y_pred_classes, target_names=target_names))"
      ],
      "metadata": {
        "id": "XmvsQuS0czpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation des performances\n",
        "plt.plot(history2_4.history['val_accuracy'], label='LSTM2')\n",
        "plt.plot(historybic3.history['val_accuracy'], label='BiLSTM')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "TN_R-3Pnc7dc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}